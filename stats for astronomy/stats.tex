\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{gensymb}
\graphicspath{{./}}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[section]{placeins}
\setlength{\parindent}{0pt}
\usepackage{url}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{enumerate}

\title{Lecture Notes Statistic for Astronomy}
\author{Mick Veldhuis}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\section{Statistics basics}

\subsection{Cox's rules}

The first rule states that the probability of something happening, let's call the event $X$, plus that of $X$ not happening is one:

\begin{equation}
	P(X)+P(\bar{X})=1
\end{equation}

The second rule (the product rule) states that the probability of events $X$ and $Y$ happening is given by:

\begin{equation}
	P(X,Y)=P(X|Y)\cdot P(Y)
\end{equation}

Where $P(X|Y)$ is the probability of $X$ given $Y$ happening.

\subsection{Bayes' theorem}

Bayes' theorem gives us the probability of the hypothesis $H$ being true given the data $D$.

\begin{equation}
	P(H|D)=\frac{P(D|H)\cdot P(H)}{P(D)}
\end{equation}

Where $P(H|D)$ is called the posterior, $P(D|H)$ the likelihood, $P(H)$ the prior, and $P(D)$ the evidence. 

\subsection{Marginalization}

Suppose you have a range of hypotheses $\{H_i\}$, where $i=0,1,2,3,\dots$. Then

\begin{equation}
	\sum_{i=0}^{N-1} P(H_i)=1
\end{equation}

Then suppose we have some nuisance parameter $X$ (quantities of no intrinsic interest, that sadly enter our analyses), then 

\begin{align}
	\sum P(H_i,X) &= \sum P(H_i|X)\cdot P(X)\\[1em]
	&= P(X)\sum P(H_i|X)\\[1em]
	&= P(X)
\end{align}

Which could also be written as

\begin{align}
	P(X)=\int_{-\infty}^{\infty} P(X,Y)\,dY
\end{align}

Suppose we now have some continuous case. Then we dive into the realm of probability density functions:

\begin{equation}
	\text{pdf}(X, Y=y) = \lim_{\delta y\rightarrow 0} \frac{P(X, y\le Y < y+\delta y)}{\delta y}
\end{equation} 

Then the probability that the value of $Y$ lies between $y_1$ and $y_2$ is given by

\begin{equation}
	P(X, y_1\le Y < y_2)=\int_{y_1}^{y_2}\text{pdf}(X, Y)\,dY
\end{equation}

Where 

\begin{equation}
	\int_{-\infty}^{\infty} P(Y|X)\,dY = 1
\end{equation}

\section{Parameter estimation}

\subsection{The best estimate}

The best estimate is the value, say $X_0$, such that the posterior is at a maximum. So

\begin{equation}
	\frac{dP}{dX}\bigg|_{X=X_0}=0\ \ \text{and}\ \  \frac{d^2P}{dX^2}\bigg|_{X=X_0}<0
\end{equation}

The reliability of our best estimate is then given by the width of the posterior about $X=X_0$. To find this width we take the Taylor expansion of the natural logarithm of the posterior about $X=X_0$:

\begin{equation}
	L=\ln\big[P(X|\{data\})\big]\quad\Rightarrow\quad L=L(X_0)+\frac{1}{2}\frac{d^2P}{dX^2}\bigg|_{X_0}(X-X_0)^2+\dots
\end{equation}

Since the first term of the expansion is constant we can discard it. Thus we only care about the quadratic term, ignoring higher order terms. Such that

\begin{equation}
	[P(X|\{data\})\approx A\exp\bigg[\frac{1}{2}\frac{d^2P}{dX^2}\bigg|_{X_0}(X-X_0)^2\bigg]
\end{equation}

For $A$ is some normalization constant. We basically approximated the posterior by the Gaussian (or normal) distribution:

\begin{equation}
	P(x|\mu, \sigma)=\frac{1}{\sigma\sqrt{2\pi}}\exp\bigg[-\frac{(x-\mu)^2}{2\sigma^2}\bigg]
\end{equation}

Such that $X=X_0\pm\sigma$, where

\begin{equation}
	\sigma = \bigg(-\frac{d^2P}{dX^2}\bigg|_{X_0}\bigg)^{-1/2}
\end{equation}

Note that for $X$ within $\pm\sigma$, the probability is $67\%$, for $\pm 2\sigma$ that is $95\%$, etc. Also note that in the case of an asymmetric posterior the best estimate is not really given by $X_0$ (also called the mode), but the actual value is more likely to be on either side of the mode. We then use the mean (or expectation value) since it takes the asymmetry into account:

\begin{equation}
	E(X)=\langle X\rangle = \int X\,P(X|\{data\})\,dX
\end{equation}  

Then there's also the median (denoted by $\mu_{1/2}$), an estimate of the middle of a distribution, which can sometimes be a reasonable approximation of the best estimate of a distribution with significant outliers. The median is such that half of the points will lie above and below it.

\subsection{Average of the Gaussian distribution}

\subsubsection*{If $\sigma=$ constant}

For a Gaussian distribution the best estimate $\mu$ is just the arithmetic average of the data points $\{x_k\}$:

\begin{equation}
	\mu_0=\frac{1}{N}\sum_{k=1}^{N}x_k
\end{equation}

And our confidence in the reliability of $\mu_0$ is taken into account as follows, such that the value of $\mu$ is given to be

\begin{equation}
	\mu = \mu_0\pm \frac{\sigma}{\sqrt{N}}
\end{equation}

\subsubsection*{If $\sigma\not=$ constant}

In this case we get a weighted average

\begin{equation}
	\mu_0=\sum_k w_kx_k \bigg/\sum_k w_k, \ \text{where} \ \ w_k=\frac{1}{\sigma_k^2}
\end{equation}

where $\sigma_k^2$ is the variance of data point $k$. Such that $\mu$ is given by

\begin{equation}
	\mu = \mu_0\pm \bigg(\sum_k w_k\bigg)^{-1/2}
\end{equation}

\subsection{The variance}

The variance of the posterior is the expectation value of the square of the deviations from the mean

\begin{equation}
	\text{Var}(X)=\big\langle (X-\mu)^2\big\rangle=\int (X-\mu)^2P(X|\{data\})\,dX
\end{equation}

Which for the Gaussian distribution reduces to

\begin{equation}
	\big\langle (X-\mu)^2\big\rangle=\sigma^2
\end{equation}

Note $\sigma$ is called the standard deviation, and when $\mu=0$ it is called the root-mean square uncertainty.

\subsection{The central limit theorem}

It is the basic idea that any peaked, symmetric distribution with tails that asymptote to zero will eventually converge to a Gaussian will eventually converge to a Gaussian distribution for $N\rightarrow\infty$. It is important to note that it will not work for all distributions, but we will get to that.

\section{Probability distributions}

\subsection{Permutations and combinations}

First off, the number of ways one can arrange $n$ objects, say coins in a line, is $n!$. Then the number of permutations

\begin{equation}
	{}^nP_m=\frac{n!}{(n-m)!}
\end{equation}

gives the number of ways one can sequentially pick $m$ objects from $n$ different objects. Lastly if the order is irrelevant we get the number of combinations

\begin{equation}
	{}^nC_m=\frac{n!}{(n-m)!m!}=\begin{pmatrix}
	n\\ m
	\end{pmatrix}
\end{equation}

which is the number of ways of selecting $m$ integers out of $n$ objects.

\subsection{Binomial distribution}

The binomial distribution gives the probability of having $r$ successes in $N$ trials. $P(\text{success})=p$ and $P(\text{failure})=1-p=q$, then

\begin{equation}
	P(r|N)=\begin{pmatrix} n\\ m \end{pmatrix}p^rq^{N-r}
\end{equation} 

For $r=0,1,2,\dots,N$. The expected value for $r$ and the variance are given to be

\begin{equation}
	\langle r\rangle = Np\quad\text{and}\quad \text{Var}(r)=Npq=Np(1-p)
\end{equation}

\subsection{Poisson distribution}

From the binomial distribution we can derive the Poisson distribution if we assume that $r<<N$. Given that we know the expected value of $r$, $\langle r\rangle=\mu$, then 

\begin{equation}
	P(r|\mu)=\frac{\mu^r e^{-\mu}}{r!}
\end{equation}

Which gives the probability of observing $r$ events when we only know the expected value. Note that the variance is also given by $\mu$. \textbf{Note that often the Poisson distribution is denoted as follows}, where $r$ is exchanged for $N$:

\begin{equation}
P(N|\mu)=\frac{\mu^N e^{-\mu}}{N!}
\end{equation}

and thus $\langle N\rangle=\mu$.

\subsection{Gaussian distribution}

Note that the Gaussian has already been discussed, but I'd like to note here that the Gaussian could've been derived from the binomial distribution. Just like the Poisson distribution we assume that $r<<N$, but also that $\mu>>1$, such that we can use Sterling's formula,

\begin{equation}
	\ln(N!)\approx N\ln N-N+\frac{1}{2}\ln(2\pi N)
\end{equation}

Using this formula one could then continue the analysis, and derive the Gaussian distribution from the binomial distribution.

\subsection{Lorentzian / Cauchy distribution}

The Lorentzian distribution is given by

\begin{equation}
	P(x|\alpha, \beta)=\frac{\beta}{\pi\big[\beta^2+(x-\alpha)^2\big]}
\end{equation}

It is symmetric with respect to the maximum, at $x=\alpha$, has a FWHM of $2\beta$, and an amplitude of $1/\pi\beta$. Note that for this distribution the expected value or rather mean does not exist, nor do the other moments such as the standard deviation.

\subsection{Location and scale parameters}



\end{document}
