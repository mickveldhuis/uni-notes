\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{gensymb}
\graphicspath{{./}}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[section]{placeins}
\setlength{\parindent}{0pt}
\usepackage{url}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{enumerate}

\title{Lecture Notes Statistic for Astronomy}
\author{Mick Veldhuis}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\section{Statistics basics}

\subsection{Cox's rules}

The first rule states that the probability of something happening, let's call the event $X$, plus that of $X$ not happening is one:

\begin{equation}
	P(X)+P(\bar{X})=1
\end{equation}

The second rule (the product rule) states that the probability of events $X$ and $Y$ happening is given by:

\begin{equation}
	P(X,Y)=P(X|Y)\cdot P(Y)
\end{equation}

Where $P(X|Y)$ is the probability of $X$ given $Y$ happening.

\subsection{Bayes' theorem}

Bayes' theorem gives us the probability of the hypothesis $H$ being true given the data $D$.

\begin{equation}
	P(H|D)=\frac{P(D|H)\cdot P(H)}{P(D)}
\end{equation}

Where $P(H|D)$ is called the posterior, $P(D|H)$ the likelihood, $P(H)$ the prior, and $P(D)$ the evidence. 

\subsection{Marginalization}

Suppose you have a range of hypotheses $\{H_i\}$, where $i=0,1,2,3,\dots$. Then

\begin{equation}
	\sum_{i=0}^{N-1} P(H_i)=1
\end{equation}

Then suppose we have some nuisance parameter $X$ (quantities of no intrinsic interest, that sadly enter our analyses), then 

\begin{align}
	\sum P(H_i,X) &= \sum P(H_i|X)\cdot P(X)\\[1em]
	&= P(X)\sum P(H_i|X)\\[1em]
	&= P(X)
\end{align}

Which could also be written as

\begin{align}
	P(X)=\int_{-\infty}^{\infty} P(X,Y)\,dY
\end{align}

Suppose we now have some continuous case. Then we dive into the realm of probability density functions:

\begin{equation}
	\text{pdf}(X, Y=y) = \lim_{\delta y\rightarrow 0} \frac{P(X, y\le Y < y+\delta y)}{\delta y}
\end{equation} 

Then the probability that the value of $Y$ lies between $y_1$ and $y_2$ is given by

\begin{equation}
	P(X, y_1\le Y < y_2)=\int_{y_1}^{y_2}\text{pdf}(X, Y)\,dY
\end{equation}

Where 

\begin{equation}
	\int_{-\infty}^{\infty} P(Y|X)\,dY = 1
\end{equation}

\section{Parameter estimation}

\subsection{The best estimate}

The best estimate is the value, say $X_0$, such that the posterior is at a maximum. So

\begin{equation}
	\frac{dP}{dX}\bigg|_{X=X_0}=0\ \ \text{and}\ \  \frac{d^2P}{dX^2}\bigg|_{X=X_0}<0
\end{equation}

The reliability of our best estimate is then given by the width of the posterior about $X=X_0$. To find this width we take the Taylor expansion of the natural logarithm of the posterior about $X=X_0$:

\begin{equation}
	L=\ln\big[P(X|\{data\})\big]\quad\Rightarrow\quad L=L(X_0)+\frac{1}{2}\frac{d^2P}{dX^2}\bigg|_{X_0}(X-X_0)^2+\dots
\end{equation}

Since the first term of the expansion is constant we can discard it. Thus we only care about the quadratic term, ignoring higher order terms. Such that

\begin{equation}
	[P(X|\{data\})\approx A\exp\bigg[\frac{1}{2}\frac{d^2P}{dX^2}\bigg|_{X_0}(X-X_0)^2\bigg]
\end{equation}

For $A$ is some normalization constant. We basically approximated the posterior by the Gaussian (or normal) distribution:

\begin{equation}
	P(x|\mu, \sigma)=\frac{1}{\sigma\sqrt{2\pi}}\exp\bigg[-\frac{(x-\mu)^2}{2\sigma^2}\bigg]
\end{equation}

Such that $X=X_0\pm\sigma$, where

\begin{equation}
	\sigma = \bigg(-\frac{d^2P}{dX^2}\bigg|_{X_0}\bigg)^{-1/2}
\end{equation}

Note that for $X$ within $\pm\sigma$, the probability is $67\%$, for $\pm 2\sigma$ that is $95\%$, etc.

\end{document}

